{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6eb1e-74dd-4b60-94d2-5c790c94baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class Detection:\n",
    "    \"\"\"Data class for object detections\"\"\"\n",
    "    bbox: List[float]  # [x1, y1, x2, y2]\n",
    "    confidence: float\n",
    "    class_id: int\n",
    "    class_name: str\n",
    "    track_id: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class PersonState:\n",
    "    \"\"\"State tracking for each person\"\"\"\n",
    "    track_id: int\n",
    "    held_objects: Dict[int, int] = None  # object_id -> frames_held\n",
    "    pose_landmarks: Optional[List] = None\n",
    "    bbox: List[float] = None\n",
    "    interaction_history: List = None\n",
    "    theft_flags: int = 0\n",
    "    last_seen: int = 0\n",
    "    \n",
    "    def _post_init_(self):\n",
    "        if self.held_objects is None:\n",
    "            self.held_objects = {}\n",
    "        if self.interaction_history is None:\n",
    "            self.interaction_history = deque(maxlen=30)\n",
    "\n",
    "class ImprovedTheftDetectionSystem:\n",
    "    \"\"\"\n",
    "    Improved Theft Detection System with better debugging and detection capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def _init_(self, \n",
    "                 yolo_model_path: str = 'yolov8n.pt',\n",
    "                 confidence_threshold: float = 0.3,  # Lowered for better detection\n",
    "                 interaction_distance_threshold: float = 150,  # Increased threshold\n",
    "                 concealment_distance_threshold: float = 100,\n",
    "                 min_interaction_frames: int = 3,  # Reduced for quicker detection\n",
    "                 theft_confirmation_frames: int = 10,\n",
    "                 debug_mode: bool = True):\n",
    "        \n",
    "        self.debug_mode = debug_mode\n",
    "        \n",
    "        # Initialize models with error handling\n",
    "        try:\n",
    "            print(\"Loading YOLOv8 model...\")\n",
    "            self.yolo_model = YOLO(yolo_model_path)\n",
    "            print(f\"✅ YOLOv8 model loaded successfully!\")\n",
    "            print(f\"Available classes: {list(self.yolo_model.names.values())}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading YOLO model: {e}\")\n",
    "            print(\"Trying to download yolov8n.pt...\")\n",
    "            self.yolo_model = YOLO('yolov8n.pt')  # This will auto-download\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading MediaPipe Pose...\")\n",
    "            self.mp_pose = mp.solutions.pose\n",
    "            self.pose = self.mp_pose.Pose(\n",
    "                static_image_mode=False,\n",
    "                model_complexity=1,\n",
    "                enable_segmentation=False,\n",
    "                min_detection_confidence=0.3,  # Lowered threshold\n",
    "                min_tracking_confidence=0.3\n",
    "            )\n",
    "            print(\"✅ MediaPipe Pose loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading MediaPipe: {e}\")\n",
    "            self.pose = None\n",
    "        \n",
    "        # Detection parameters\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.interaction_distance_threshold = interaction_distance_threshold\n",
    "        self.concealment_distance_threshold = concealment_distance_threshold\n",
    "        self.min_interaction_frames = min_interaction_frames\n",
    "        self.theft_confirmation_frames = theft_confirmation_frames\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.person_states: Dict[int, PersonState] = {}\n",
    "        self.object_tracks: Dict[int, Dict] = {}\n",
    "        self.frame_count = 0\n",
    "        self.theft_alerts = []\n",
    "        self.detection_stats = {\n",
    "            'total_persons': 0,\n",
    "            'total_objects': 0,\n",
    "            'total_interactions': 0,\n",
    "            'total_thefts': 0\n",
    "        }\n",
    "        \n",
    "        # Extended list of target items for supermarket theft detection\n",
    "        self.target_classes = [\n",
    "            'bottle', 'cup', 'wine glass', 'fork', 'knife', 'spoon', 'bowl',\n",
    "            'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', \n",
    "            'hot dog', 'pizza', 'donut', 'cake', 'cell phone', 'book', \n",
    "            'clock', 'scissors', 'teddy bear', 'hair drier', 'toothbrush',\n",
    "            'laptop', 'mouse', 'remote', 'keyboard', 'sports ball',\n",
    "            'frisbee', 'skateboard', 'surfboard', 'tennis racket'\n",
    "        ]\n",
    "        \n",
    "        print(f\"🎯 Target classes for theft detection: {self.target_classes}\")\n",
    "        print(\"🚀 Theft Detection System initialized successfully!\")\n",
    "    \n",
    "    def detect_objects(self, frame: np.ndarray) -> List[Detection]:\n",
    "        \"\"\"Detect objects using YOLOv8 with improved error handling\"\"\"\n",
    "        try:\n",
    "            results = self.yolo_model(frame, verbose=False, conf=self.confidence_threshold)\n",
    "            detections = []\n",
    "            \n",
    "            for result in results:\n",
    "                boxes = result.boxes\n",
    "                if boxes is not None and len(boxes) > 0:\n",
    "                    for box in boxes:\n",
    "                        try:\n",
    "                            # Extract detection data\n",
    "                            bbox = box.xyxy[0].cpu().numpy().tolist()  # [x1, y1, x2, y2]\n",
    "                            confidence = float(box.conf[0].cpu().numpy())\n",
    "                            class_id = int(box.cls[0].cpu().numpy())\n",
    "                            class_name = self.yolo_model.names[class_id]\n",
    "                            \n",
    "                            detection = Detection(\n",
    "                                bbox=bbox,\n",
    "                                confidence=confidence,\n",
    "                                class_id=class_id,\n",
    "                                class_name=class_name\n",
    "                            )\n",
    "                            detections.append(detection)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            if self.debug_mode:\n",
    "                                print(f\"Error processing detection: {e}\")\n",
    "                            continue\n",
    "            \n",
    "            # Debug output\n",
    "            if self.debug_mode and self.frame_count % 30 == 0:  # Every 30 frames\n",
    "                persons = [d for d in detections if d.class_name == 'person']\n",
    "                objects = [d for d in detections if d.class_name in self.target_classes]\n",
    "                print(f\"Frame {self.frame_count}: Detected {len(persons)} persons, {len(objects)} target objects\")\n",
    "                if objects:\n",
    "                    obj_names = [f\"{obj.class_name}({obj.confidence:.2f})\" for obj in objects]\n",
    "                    print(f\"  Objects: {obj_names}\")\n",
    "            \n",
    "            return detections\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in object detection: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def simple_tracking(self, detections: List[Detection]) -> List[Detection]:\n",
    "        \"\"\"Improved tracking with better ID assignment\"\"\"\n",
    "        # Separate persons and objects\n",
    "        persons = [d for d in detections if d.class_name == 'person']\n",
    "        objects = [d for d in detections if d.class_name in self.target_classes]\n",
    "        \n",
    "        # Track persons\n",
    "        tracked_persons = self._track_entities(persons, 'person')\n",
    "        \n",
    "        # Track objects\n",
    "        tracked_objects = self._track_entities(objects, 'object')\n",
    "        \n",
    "        # Update detection stats\n",
    "        self.detection_stats['total_persons'] = len(tracked_persons)\n",
    "        self.detection_stats['total_objects'] = len(tracked_objects)\n",
    "        \n",
    "        return tracked_persons + tracked_objects\n",
    "    \n",
    "    def _track_entities(self, detections: List[Detection], entity_type: str) -> List[Detection]:\n",
    "        \"\"\"Enhanced tracking with better matching\"\"\"\n",
    "        if entity_type == 'person':\n",
    "            existing_tracks = {k: v for k, v in self.person_states.items() \n",
    "                             if self.frame_count - v.last_seen < 30}  # Clean old tracks\n",
    "        else:\n",
    "            existing_tracks = {k: v for k, v in self.object_tracks.items() \n",
    "                             if self.frame_count - v.get('last_seen', 0) < 20}\n",
    "        \n",
    "        tracked_detections = []\n",
    "        used_track_ids = set()\n",
    "        \n",
    "        for detection in detections:\n",
    "            best_match_id = None\n",
    "            best_score = 0\n",
    "            \n",
    "            # Find best matching existing track\n",
    "            for track_id, track_info in existing_tracks.items():\n",
    "                if track_id in used_track_ids:\n",
    "                    continue\n",
    "                \n",
    "                if entity_type == 'person':\n",
    "                    last_bbox = track_info.bbox if track_info.bbox else [0, 0, 0, 0]\n",
    "                else:\n",
    "                    last_bbox = track_info.get('bbox', [0, 0, 0, 0])\n",
    "                \n",
    "                # Calculate IoU and distance-based score\n",
    "                iou = self._calculate_iou(detection.bbox, last_bbox)\n",
    "                distance = self._calculate_distance(\n",
    "                    self._get_bbox_center(detection.bbox), \n",
    "                    self._get_bbox_center(last_bbox)\n",
    "                )\n",
    "                \n",
    "                # Combined score (IoU + distance factor)\n",
    "                score = iou + (1.0 / (1.0 + distance / 100.0)) * 0.3\n",
    "                \n",
    "                if score > best_score and iou > 0.1:  # Lower IoU threshold\n",
    "                    best_score = score\n",
    "                    best_match_id = track_id\n",
    "            \n",
    "            # Assign track ID\n",
    "            if best_match_id is not None:\n",
    "                detection.track_id = best_match_id\n",
    "                used_track_ids.add(best_match_id)\n",
    "            else:\n",
    "                # Create new track\n",
    "                existing_ids = list(existing_tracks.keys()) + list(self.person_states.keys()) + list(self.object_tracks.keys())\n",
    "                new_id = max(existing_ids + [0]) + 1\n",
    "                detection.track_id = new_id\n",
    "                \n",
    "                if entity_type == 'person':\n",
    "                    self.person_states[new_id] = PersonState(track_id=new_id)\n",
    "                else:\n",
    "                    self.object_tracks[new_id] = {}\n",
    "            \n",
    "            # Update track information\n",
    "            if entity_type == 'person':\n",
    "                self.person_states[detection.track_id].bbox = detection.bbox\n",
    "                self.person_states[detection.track_id].last_seen = self.frame_count\n",
    "            else:\n",
    "                self.object_tracks[detection.track_id]['bbox'] = detection.bbox\n",
    "                self.object_tracks[detection.track_id]['last_seen'] = self.frame_count\n",
    "                self.object_tracks[detection.track_id]['class_name'] = detection.class_name\n",
    "                self.object_tracks[detection.track_id]['confidence'] = detection.confidence\n",
    "            \n",
    "            tracked_detections.append(detection)\n",
    "        \n",
    "        return tracked_detections\n",
    "    \n",
    "    def _calculate_iou(self, box1: List[float], box2: List[float]) -> float:\n",
    "        \"\"\"Calculate Intersection over Union (IoU) of two bounding boxes\"\"\"\n",
    "        try:\n",
    "            x1 = max(box1[0], box2[0])\n",
    "            y1 = max(box1[1], box2[1])\n",
    "            x2 = min(box1[2], box2[2])\n",
    "            y2 = min(box1[3], box2[3])\n",
    "            \n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                return 0\n",
    "            \n",
    "            intersection = (x2 - x1) * (y2 - y1)\n",
    "            area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "            area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "            union = area1 + area2 - intersection\n",
    "            \n",
    "            return intersection / union if union > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def estimate_pose(self, frame: np.ndarray, person_bbox: List[float]) -> Optional[List]:\n",
    "        \"\"\"Enhanced pose estimation with error handling\"\"\"\n",
    "        if self.pose is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Extract person region with padding\n",
    "            x1, y1, x2, y2 = map(int, person_bbox)\n",
    "            h, w = frame.shape[:2]\n",
    "            \n",
    "            # Add padding and ensure bounds\n",
    "            padding = 20\n",
    "            x1 = max(0, x1 - padding)\n",
    "            y1 = max(0, y1 - padding)\n",
    "            x2 = min(w, x2 + padding)\n",
    "            y2 = min(h, y2 + padding)\n",
    "            \n",
    "            person_roi = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            if person_roi.size == 0 or person_roi.shape[0] < 10 or person_roi.shape[1] < 10:\n",
    "                return None\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            rgb_roi = cv2.cvtColor(person_roi, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Perform pose estimation\n",
    "            results = self.pose.process(rgb_roi)\n",
    "            \n",
    "            if results.pose_landmarks:\n",
    "                # Convert normalized coordinates to frame coordinates\n",
    "                landmarks = []\n",
    "                for landmark in results.pose_landmarks.landmark:\n",
    "                    # Convert relative coordinates to absolute\n",
    "                    abs_x = landmark.x * (x2 - x1) + x1\n",
    "                    abs_y = landmark.y * (y2 - y1) + y1\n",
    "                    landmarks.append([abs_x, abs_y, landmark.visibility])\n",
    "                \n",
    "                return landmarks\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Pose estimation error: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def analyze_interactions(self, persons: List[Detection], objects: List[Detection]):\n",
    "        \"\"\"Enhanced interaction analysis with better theft detection logic\"\"\"\n",
    "        \n",
    "        if self.debug_mode and persons and objects:\n",
    "            print(f\"Analyzing interactions: {len(persons)} persons, {len(objects)} objects\")\n",
    "        \n",
    "        for person in persons:\n",
    "            person_state = self.person_states[person.track_id]\n",
    "            \n",
    "            # Update pose landmarks\n",
    "            if self.pose is not None:\n",
    "                pose_landmarks = self.estimate_pose(self.current_frame, person.bbox)\n",
    "                person_state.pose_landmarks = pose_landmarks\n",
    "            \n",
    "            # Check interactions with objects\n",
    "            interaction_count = 0\n",
    "            for obj in objects:\n",
    "                if self._check_person_object_interaction(person, obj, person_state):\n",
    "                    interaction_count += 1\n",
    "            \n",
    "            # Check for concealment (objects that disappeared)\n",
    "            concealment_detected = self._check_for_concealment(person, person_state, objects)\n",
    "            \n",
    "            if interaction_count > 0:\n",
    "                self.detection_stats['total_interactions'] += interaction_count\n",
    "                \n",
    "            if concealment_detected:\n",
    "                self.detection_stats['total_thefts'] += 1\n",
    "    \n",
    "    def _check_person_object_interaction(self, person: Detection, obj: Detection, person_state: PersonState) -> bool:\n",
    "        \"\"\"Enhanced interaction detection with multiple criteria\"\"\"\n",
    "        \n",
    "        # Calculate distance between person center and object center\n",
    "        person_center = self._get_bbox_center(person.bbox)\n",
    "        object_center = self._get_bbox_center(obj.bbox)\n",
    "        distance = self._calculate_distance(person_center, object_center)\n",
    "        \n",
    "        # Check if bounding boxes overlap\n",
    "        bbox_overlap = self._check_bbox_overlap(person.bbox, obj.bbox)\n",
    "        \n",
    "        # Check if hands are near the object (if pose is available)\n",
    "        hand_near_object = False\n",
    "        if person_state.pose_landmarks and len(person_state.pose_landmarks) > 16:\n",
    "            # Get wrist landmarks (15: left wrist, 16: right wrist)\n",
    "            left_wrist = person_state.pose_landmarks[15]\n",
    "            right_wrist = person_state.pose_landmarks[16]\n",
    "            \n",
    "            for wrist in [left_wrist, right_wrist]:\n",
    "                if wrist[2] > 0.3:  # Check visibility (lowered threshold)\n",
    "                    wrist_distance = self._calculate_distance([wrist[0], wrist[1]], object_center)\n",
    "                    if wrist_distance < self.interaction_distance_threshold:\n",
    "                        hand_near_object = True\n",
    "                        break\n",
    "        \n",
    "        # Determine interaction (more lenient criteria)\n",
    "        is_interacting = (distance < self.interaction_distance_threshold * 1.5) or bbox_overlap or hand_near_object\n",
    "        \n",
    "        if is_interacting:\n",
    "            if obj.track_id not in person_state.held_objects:\n",
    "                person_state.held_objects[obj.track_id] = 0\n",
    "                if self.debug_mode:\n",
    "                    print(f\"👋 Person {person.track_id} started interacting with {obj.class_name} {obj.track_id}\")\n",
    "            \n",
    "            person_state.held_objects[obj.track_id] += 1\n",
    "            \n",
    "            # Log interaction\n",
    "            person_state.interaction_history.append({\n",
    "                'frame': self.frame_count,\n",
    "                'object_id': obj.track_id,\n",
    "                'object_class': obj.class_name,\n",
    "                'distance': distance,\n",
    "                'hand_near': hand_near_object,\n",
    "                'bbox_overlap': bbox_overlap\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _check_bbox_overlap(self, bbox1: List[float], bbox2: List[float]) -> bool:\n",
    "        \"\"\"Check if two bounding boxes overlap\"\"\"\n",
    "        return not (bbox1[2] < bbox2[0] or bbox2[2] < bbox1[0] or \n",
    "                   bbox1[3] < bbox2[1] or bbox2[3] < bbox1[1])\n",
    "    \n",
    "    def _check_for_concealment(self, person: Detection, person_state: PersonState, current_objects: List[Detection]) -> bool:\n",
    "        \"\"\"Enhanced concealment detection with better logic\"\"\"\n",
    "        \n",
    "        # Get current object IDs that are still visible\n",
    "        current_object_ids = set(obj.track_id for obj in current_objects)\n",
    "        concealment_detected = False\n",
    "        \n",
    "        # Check held objects that are no longer visible\n",
    "        for obj_id in list(person_state.held_objects.keys()):\n",
    "            interaction_frames = person_state.held_objects[obj_id]\n",
    "            \n",
    "            if obj_id not in current_object_ids:\n",
    "                # Object has disappeared - check if it was concealed\n",
    "                if interaction_frames >= self.min_interaction_frames:\n",
    "                    # Get object info\n",
    "                    obj_info = self.object_tracks.get(obj_id, {})\n",
    "                    object_class = obj_info.get('class_name', 'unknown')\n",
    "                    confidence = obj_info.get('confidence', 0.5)\n",
    "                    \n",
    "                    # Check concealment gesture\n",
    "                    concealment_confidence = self._analyze_concealment_gesture(person, person_state)\n",
    "                    \n",
    "                    # Lower threshold for detection\n",
    "                    if concealment_confidence > 0.3:  # More lenient threshold\n",
    "                        person_state.theft_flags += 1\n",
    "                        concealment_detected = True\n",
    "                        \n",
    "                        alert = {\n",
    "                            'frame': self.frame_count,\n",
    "                            'person_id': person.track_id,\n",
    "                            'object_id': obj_id,\n",
    "                            'object_class': object_class,\n",
    "                            'interaction_frames': interaction_frames,\n",
    "                            'concealment_confidence': concealment_confidence,\n",
    "                            'detection_confidence': confidence,\n",
    "                            'bbox': person.bbox.copy()\n",
    "                        }\n",
    "                        self.theft_alerts.append(alert)\n",
    "                        \n",
    "                        print(f\"🚨 THEFT DETECTED! Person {person.track_id} concealed {object_class} at frame {self.frame_count}\")\n",
    "                        print(f\"   Interaction frames: {interaction_frames}, Concealment confidence: {concealment_confidence:.2f}\")\n",
    "                \n",
    "                # Remove the object from held_objects\n",
    "                del person_state.held_objects[obj_id]\n",
    "        \n",
    "        return concealment_detected\n",
    "    \n",
    "    def _analyze_concealment_gesture(self, person: Detection, person_state: PersonState) -> float:\n",
    "        \"\"\"Analyze concealment gesture and return confidence score\"\"\"\n",
    "        \n",
    "        base_confidence = 0.5  # Base confidence if no pose data\n",
    "        \n",
    "        if not person_state.pose_landmarks or len(person_state.pose_landmarks) < 25:\n",
    "            return base_confidence\n",
    "        \n",
    "        try:\n",
    "            # Get relevant body landmarks\n",
    "            left_wrist = person_state.pose_landmarks[15]\n",
    "            right_wrist = person_state.pose_landmarks[16]\n",
    "            left_hip = person_state.pose_landmarks[23]\n",
    "            right_hip = person_state.pose_landmarks[24]\n",
    "            \n",
    "            concealment_indicators = 0\n",
    "            total_checks = 0\n",
    "            \n",
    "            # Check if hands are near torso/hip area\n",
    "            for wrist in [left_wrist, right_wrist]:\n",
    "                if wrist[2] > 0.3:  # Check visibility\n",
    "                    for hip in [left_hip, right_hip]:\n",
    "                        if hip[2] > 0.3:\n",
    "                            distance = self._calculate_distance([wrist[0], wrist[1]], [hip[0], hip[1]])\n",
    "                            total_checks += 1\n",
    "                            if distance < self.concealment_distance_threshold:\n",
    "                                concealment_indicators += 1\n",
    "            \n",
    "            # Calculate confidence based on gesture analysis\n",
    "            if total_checks > 0:\n",
    "                gesture_confidence = concealment_indicators / total_checks\n",
    "                return max(base_confidence, gesture_confidence)\n",
    "            else:\n",
    "                return base_confidence\n",
    "                \n",
    "        except Exception as e:\n",
    "            if self.debug_mode:\n",
    "                print(f\"Concealment analysis error: {e}\")\n",
    "            return base_confidence\n",
    "    \n",
    "    def _get_bbox_center(self, bbox: List[float]) -> List[float]:\n",
    "        \"\"\"Get center point of bounding box\"\"\"\n",
    "        return [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "    \n",
    "    def _calculate_distance(self, point1: List[float], point2: List[float]) -> float:\n",
    "        \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "        return math.sqrt((point1[0] - point2[0])*2 + (point1[1] - point2[1])*2)\n",
    "    \n",
    "    def draw_detections(self, frame: np.ndarray, persons: List[Detection], objects: List[Detection]) -> np.ndarray:\n",
    "        \"\"\"Enhanced visualization with better debugging info\"\"\"\n",
    "        \n",
    "        # Draw persons with detailed status\n",
    "        for person in persons:\n",
    "            person_state = self.person_states.get(person.track_id)\n",
    "            color = (0, 255, 0)  # Green by default\n",
    "            thickness = 2\n",
    "            \n",
    "            # Change color and thickness based on status\n",
    "            if person_state and person_state.theft_flags > 0:\n",
    "                color = (0, 0, 255)  # Red for theft\n",
    "                thickness = 3\n",
    "            elif person_state and person_state.held_objects:\n",
    "                color = (0, 255, 255)  # Yellow for interaction\n",
    "                thickness = 2\n",
    "            \n",
    "            # Draw bounding box\n",
    "            x1, y1, x2, y2 = map(int, person.bbox)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
    "            \n",
    "            # Draw detailed label\n",
    "            label_parts = [f\"Person {person.track_id}\"]\n",
    "            \n",
    "            if person_state:\n",
    "                if person_state.theft_flags > 0:\n",
    "                    label_parts.append(\"[THEFT DETECTED!]\")\n",
    "                if person_state.held_objects:\n",
    "                    held_items = [self.object_tracks.get(obj_id, {}).get('class_name', f'obj_{obj_id}') \n",
    "                                 for obj_id in person_state.held_objects.keys()]\n",
    "                    label_parts.append(f\"[Holding: {', '.join(held_items)}]\")\n",
    "            \n",
    "            label = \" \".join(label_parts)\n",
    "            \n",
    "            # Multi-line label if too long\n",
    "            if len(label) > 50:\n",
    "                lines = [label[:50], label[50:]]\n",
    "            else:\n",
    "                lines = [label]\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                cv2.putText(frame, line, (x1, y1-10-i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            \n",
    "            # Draw pose landmarks if available\n",
    "            if person_state and person_state.pose_landmarks:\n",
    "                self._draw_pose_landmarks(frame, person_state.pose_landmarks)\n",
    "        \n",
    "        # Draw objects with confidence scores\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = map(int, obj.bbox)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue\n",
    "            \n",
    "            label = f\"{obj.class_name} {obj.track_id} ({obj.confidence:.2f})\"\n",
    "            cv2.putText(frame, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        # Draw theft alerts with animation\n",
    "        if self.theft_alerts:\n",
    "            latest_alert = self.theft_alerts[-1]\n",
    "            frames_since_alert = self.frame_count - latest_alert['frame']\n",
    "            if frames_since_alert < 90:  # Show for 3 seconds at 30fps\n",
    "                # Blinking effect\n",
    "                if frames_since_alert % 20 < 10:\n",
    "                    cv2.putText(frame, \"⚠️ THEFT DETECTED! ⚠️\", (50, 50), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "                    \n",
    "                    # Show details\n",
    "                    details = f\"Person {latest_alert['person_id']} - {latest_alert['object_class']}\"\n",
    "                    cv2.putText(frame, details, (50, 90), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "        \n",
    "        # Enhanced status information\n",
    "        info_lines = [\n",
    "            f\"Frame: {self.frame_count}\",\n",
    "            f\"Persons: {len(persons)} | Objects: {len(objects)}\",\n",
    "            f\"Interactions: {self.detection_stats['total_interactions']} | Thefts: {len(self.theft_alerts)}\",\n",
    "            f\"Confidence Threshold: {self.confidence_threshold}\",\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(info_lines):\n",
    "            y_pos = frame.shape[0] - 80 + i * 20\n",
    "            cv2.putText(frame, line, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _draw_pose_landmarks(self, frame: np.ndarray, landmarks: List):\n",
    "        \"\"\"Draw key pose landmarks\"\"\"\n",
    "        # Draw key points (hands and hips)\n",
    "        key_points = [15, 16, 23, 24]  # Left wrist, Right wrist, Left hip, Right hip\n",
    "        colors = [(0, 255, 0), (0, 255, 0), (255, 0, 255), (255, 0, 255)]\n",
    "        \n",
    "        for idx, color in zip(key_points, colors):\n",
    "            if idx < len(landmarks) and landmarks[idx][2] > 0.3:  # Check visibility\n",
    "                x, y = int(landmarks[idx][0]), int(landmarks[idx][1])\n",
    "                cv2.circle(frame, (x, y), 6, color, -1)\n",
    "    \n",
    "    def process_video(self, video_path: str, output_path: str = None, display: bool = True, skip_frames: int = 0):\n",
    "        \"\"\"Process video with enhanced error handling and performance\"\"\"\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"❌ Error: Could not open video {video_path}\")\n",
    "            return\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"📹 Processing video: {width}x{height} @ {fps} FPS, Total frames: {total_frames}\")\n",
    "        \n",
    "        # Setup video writer if output path provided\n",
    "        out = None\n",
    "        if output_path:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "            print(f\"💾 Output will be saved to: {output_path}\")\n",
    "        \n",
    "        self.frame_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Skip frames for performance if needed\n",
    "                if skip_frames > 0 and self.frame_count % (skip_frames + 1) != 0:\n",
    "                    self.frame_count += 1\n",
    "                    continue\n",
    "                \n",
    "                self.current_frame = frame\n",
    "                \n",
    "                # Detect objects\n",
    "                detections = self.detect_objects(frame)\n",
    "                \n",
    "                # Track objects\n",
    "                tracked_detections = self.simple_tracking(detections)\n",
    "                \n",
    "                # Separate persons and objects\n",
    "                persons = [d for d in tracked_detections if d.class_name == 'person']\n",
    "                objects = [d for d in tracked_detections if d.class_name in self.target_classes]\n",
    "                self.current_objects = objects\n",
    "                \n",
    "                # Analyze interactions and detect theft\n",
    "                if persons and objects:  # Only analyze if both persons and objects are present\n",
    "                    self.analyze_interactions(persons, objects)\n",
    "                \n",
    "                # Draw results\n",
    "                output_frame = self.draw_detections(frame, persons, objects)\n",
    "                \n",
    "                # Write frame if output specified\n",
    "                if out:\n",
    "                    out.write(output_frame)\n",
    "                \n",
    "                # Display frame\n",
    "                if display:\n",
    "                    cv2.imshow('Enhanced Theft Detection System', output_frame)\n",
    "                    \n",
    "                    key = cv2.waitKey(1) & 0xFF\n",
    "                    if key == ord('q'):\n",
    "                        print(\"👋 Stopping video processing...\")\n",
    "                        break\n",
    "                    elif key == ord('s'):\n",
    "                        # Save screenshot\n",
    "                        screenshot_name = f'theft_detection_frame_{self.frame_count}.jpg'\n",
    "                        cv2.imwrite(screenshot_name, output_frame)\n",
    "                        print(f\"📸 Screenshot saved: {screenshot_name}\")\n",
    "                    elif key == ord('d'):\n",
    "                        # Toggle debug mode\n",
    "                        self.debug_mode = not self.debug_mode\n",
    "                        print(f\"🔧 Debug mode: {'ON' if self.debug_mode else 'OFF'}\")\n",
    "                \n",
    "                self.frame_count += 1\n",
    "                \n",
    "                # Print progress\n",
    "                if self.frame_count % 100 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    fps_current = self.frame_count / elapsed\n",
    "                    progress = (self.frame_count / total_frames) * 100\n",
    "                    print(f\"📊 Progress: {progress:.1f}% | FPS: {fps_current:.1f} | Thefts: {len(self.theft_alerts)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during video processing: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            cap.release()\n",
    "            if out:\n",
    "                out.release()\n",
    "            if display:\n",
    "                cv2.destroyAllWindows()\n",
    "            \n",
    "            # Print final results\n",
    "            elapsed_total = time.time() - start_time\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"🎯 THEFT DETECTION RESULTS\")\n",
    "            print(f\"{'='*50}\")\n",
    "            print(f\"📹 Total frames processed: {self.frame_count}\")\n",
    "            print(f\"⏱️  Total processing time: {elapsed_total:.2f} seconds\")\n",
    "            print(f\"🎬 Average FPS: {self.frame_count / elapsed_total:.2f}\")\n",
    "            print(f\"👥 Peak persons detected: {self.detection_stats['total_persons']}\")\n",
    "            print(f\"📦 Peak objects detected: {self.detection_stats['total_objects']}\")\n",
    "            print(f\"🤝 Total interactions: {self.detection_stats['total_interactions']}\")\n",
    "            print(f\"🚨 Total theft alerts: {len(self.theft_alerts)}\")\n",
    "            \n",
    "            if self.theft_alerts:\n",
    "                print(f\"\\n📋 DETAILED THEFT EVENTS:\")\n",
    "                for i, alert in enumerate(self.theft_alerts, 1):\n",
    "                    print(f\"  {i}. Frame {alert['frame']:6d}: Person {alert['person_id']} concealed {alert['object_class']}\")\n",
    "                    print(f\"     Confidence: {alert['concealment_confidence']:.2f} | Interaction frames: {alert['interaction_frames']}\")\n",
    "            else:\n",
    "                print(f\"✅ No theft detected in this video.\")\n",
    "    \n",
    "    def process_webcam(self, camera_id: int = 0):\n",
    "        \"\"\"Process live webcam feed for real-time theft detection\"\"\"\n",
    "        \n",
    "        cap = cv2.VideoCapture(camera_id)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"❌ Error: Could not open camera {camera_id}\")\n",
    "            return\n",
    "        \n",
    "        # Set camera properties for better performance\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        \n",
    "        print(\"🎥 Starting real-time theft detection from webcam...\")\n",
    "        print(\"Controls: 'q' to quit, 's' to save screenshot, 'd' to toggle debug\")\n",
    "        \n",
    "        self.frame_count = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"❌ Failed to capture frame\")\n",
    "                    break\n",
    "                \n",
    "                self.current_frame = frame\n",
    "                \n",
    "                # Detect and track\n",
    "                detections = self.detect_objects(frame)\n",
    "                tracked_detections = self.simple_tracking(detections)\n",
    "                \n",
    "                # Separate persons and objects\n",
    "                persons = [d for d in tracked_detections if d.class_name == 'person']\n",
    "                objects = [d for d in tracked_detections if d.class_name in self.target_classes]\n",
    "                self.current_objects = objects\n",
    "                \n",
    "                # Analyze interactions\n",
    "                if persons and objects:\n",
    "                    self.analyze_interactions(persons, objects)\n",
    "                \n",
    "                # Draw and display\n",
    "                output_frame = self.draw_detections(frame, persons, objects)\n",
    "                cv2.imshow('Real-time Theft Detection', output_frame)\n",
    "                \n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    screenshot_name = f'realtime_theft_detection_{int(time.time())}.jpg'\n",
    "                    cv2.imwrite(screenshot_name, output_frame)\n",
    "                    print(f\"📸 Screenshot saved: {screenshot_name}\")\n",
    "                elif key == ord('d'):\n",
    "                    self.debug_mode = not self.debug_mode\n",
    "                    print(f\"🔧 Debug mode: {'ON' if self.debug_mode else 'OFF'}\")\n",
    "                \n",
    "                self.frame_count += 1\n",
    "                \n",
    "                # Print stats every 5 seconds\n",
    "                if self.frame_count % 150 == 0:  # Assuming 30 FPS\n",
    "                    elapsed = time.time() - start_time\n",
    "                    fps_current = self.frame_count / elapsed\n",
    "                    print(f\"📊 Live Stats - FPS: {fps_current:.1f} | Persons: {len(persons)} | Objects: {len(objects)} | Thefts: {len(self.theft_alerts)}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during webcam processing: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "    \n",
    "    def test_detection_on_frame(self, frame_path: str):\n",
    "        \"\"\"Test detection capabilities on a single frame\"\"\"\n",
    "        print(f\"🧪 Testing detection on frame: {frame_path}\")\n",
    "        \n",
    "        frame = cv2.imread(frame_path)\n",
    "        if frame is None:\n",
    "            print(f\"❌ Could not load image: {frame_path}\")\n",
    "            return\n",
    "        \n",
    "        # Detect objects\n",
    "        detections = self.detect_objects(frame)\n",
    "        \n",
    "        print(f\"📊 Detection Results:\")\n",
    "        print(f\"  Total detections: {len(detections)}\")\n",
    "        \n",
    "        persons = [d for d in detections if d.class_name == 'person']\n",
    "        objects = [d for d in detections if d.class_name in self.target_classes]\n",
    "        \n",
    "        print(f\"  Persons: {len(persons)}\")\n",
    "        print(f\"  Target objects: {len(objects)}\")\n",
    "        \n",
    "        if persons:\n",
    "            print(\"  Person details:\")\n",
    "            for person in persons:\n",
    "                print(f\"    - Person: confidence={person.confidence:.2f}, bbox={person.bbox}\")\n",
    "        \n",
    "        if objects:\n",
    "            print(\"  Object details:\")\n",
    "            for obj in objects:\n",
    "                print(f\"    - {obj.class_name}: confidence={obj.confidence:.2f}, bbox={obj.bbox}\")\n",
    "        \n",
    "        # Draw detections\n",
    "        output_frame = frame.copy()\n",
    "        for person in persons:\n",
    "            x1, y1, x2, y2 = map(int, person.bbox)\n",
    "            cv2.rectangle(output_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(output_frame, f\"Person {person.confidence:.2f}\", (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = map(int, obj.bbox)\n",
    "            cv2.rectangle(output_frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(output_frame, f\"{obj.class_name} {obj.confidence:.2f}\", (x1, y1-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "        \n",
    "        # Display result\n",
    "        cv2.imshow('Detection Test', output_frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        # Save test result\n",
    "        test_output_path = f\"detection_test_{int(time.time())}.jpg\"\n",
    "        cv2.imwrite(test_output_path, output_frame)\n",
    "        print(f\"💾 Test result saved to: {test_output_path}\")\n",
    "\n",
    "# Debugging and troubleshooting functions\n",
    "def diagnose_system():\n",
    "    \"\"\"Diagnose common issues with the theft detection system\"\"\"\n",
    "    print(\"🔍 SYSTEM DIAGNOSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check PyTorch\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"✅ PyTorch: {torch._version_}\")\n",
    "        print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"   CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch not installed\")\n",
    "    \n",
    "    # Check OpenCV\n",
    "    try:\n",
    "        import cv2\n",
    "        print(f\"✅ OpenCV: {cv2._version_}\")\n",
    "    except ImportError:\n",
    "        print(\"❌ OpenCV not installed\")\n",
    "    \n",
    "    # Check Ultralytics\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        print(\"✅ Ultralytics YOLO available\")\n",
    "    except ImportError:\n",
    "        print(\"❌ Ultralytics not installed\")\n",
    "    \n",
    "    # Check MediaPipe\n",
    "    try:\n",
    "        import mediapipe as mp\n",
    "        print(\"✅ MediaPipe available\")\n",
    "    except ImportError:\n",
    "        print(\"❌ MediaPipe not installed\")\n",
    "    \n",
    "    print(\"\\n📋 INSTALLATION COMMANDS:\")\n",
    "    print(\"pip install ultralytics opencv-python mediapipe torch numpy\")\n",
    "    \n",
    "    # Test YOLO model download\n",
    "    try:\n",
    "        print(\"\\n🔄 Testing YOLO model download...\")\n",
    "        model = YOLO('yolov8n.pt')\n",
    "        print(\"✅ YOLO model loaded successfully\")\n",
    "        print(f\"   Available classes: {len(model.names)} classes\")\n",
    "        print(f\"   Sample classes: {list(model.names.values())[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ YOLO model test failed: {e}\")\n",
    "\n",
    "# Enhanced example usage\n",
    "def main():\n",
    "    \"\"\"Enhanced example usage with debugging options\"\"\"\n",
    "    \n",
    "    print(\"🚀 ENHANCED THEFT DETECTION SYSTEM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Run system diagnosis first\n",
    "    diagnose_system()\n",
    "    \n",
    "    # Initialize the system with debugging enabled\n",
    "    detector = ImprovedTheftDetectionSystem(\n",
    "        confidence_threshold=0.3,  # Lower threshold for better detection\n",
    "        interaction_distance_threshold=150,  # Larger interaction zone\n",
    "        concealment_distance_threshold=100,\n",
    "        min_interaction_frames=2,  # Faster detection\n",
    "        theft_confirmation_frames=8,\n",
    "        debug_mode=True  # Enable debugging\n",
    "    )\n",
    "    \n",
    "    # Option 1: Test with a single frame first\n",
    "    test_frame_path = \"test_frame.jpg\"  # Replace with your test image\n",
    "    try:\n",
    "        detector.test_detection_on_frame(test_frame_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping frame test: {e}\")\n",
    "    \n",
    "    # Option 2: Process a video file\n",
    "    video_path = \"supermarket_theft_video.mp4\"  # Replace with your video path\n",
    "    output_path = \"theft_detection_output.mp4\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🎬 Starting video processing...\")\n",
    "        detector.process_video(\n",
    "            video_path=video_path, \n",
    "            output_path=output_path, \n",
    "            display=True,\n",
    "            skip_frames=0  # Process all frames, set to 1 to skip every other frame for speed\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Video file not found: {video_path}\")\n",
    "        print(\"🎥 Trying webcam instead...\")\n",
    "        \n",
    "        # Option 3: Process webcam if video file not found\n",
    "        try:\n",
    "            detector.process_webcam(camera_id=0)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Webcam error: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing video: {e}\")\n",
    "        print(\"💡 Try reducing the confidence threshold or checking your video format\")\n",
    "\n",
    "# Additional utility function for batch processing\n",
    "def process_multiple_videos(video_paths: List[str], output_dir: str = \"theft_detection_outputs\"):\n",
    "    \"\"\"Process multiple videos in batch\"\"\"\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    detector = ImprovedTheftDetectionSystem(debug_mode=True)\n",
    "    \n",
    "    for i, video_path in enumerate(video_paths, 1):\n",
    "        print(f\"\\n🎬 Processing video {i}/{len(video_paths)}: {video_path}\")\n",
    "        \n",
    "        video_name = os.path.basename(video_path).split('.')[0]\n",
    "        output_path = os.path.join(output_dir, f\"{video_name}_theft_detection.mp4\")\n",
    "        \n",
    "        try:\n",
    "            detector.process_video(video_path, output_path, display=False)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {video_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
